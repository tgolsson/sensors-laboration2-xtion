\documentclass[11pt]{article}

%% Separate file for preamble with macros and stuff
\include{preamble} 

%% For make-title
\title{Laboration 2: RGBD-cameras\\ {\small Sensors and Sensing}}
\author{Marek Beƒçica, Tom Olsson}
\date{\today}

\begin{document}
\maketitle %Title area
\begin{center}
  \emph{All code for this exercise can be found at \\ \url{https://github.com/tgolsson/sensors-laboration2-xtion}}
\end{center}
\tableofcontents
\lstlistoflistings % List of all code snippets
\listoffigures % List of all figures
\listoftables
\lstset{ matchrangestart=t} %initialise the linerange-macro for \lstinput...
\section{Theory and motivation}
\subsection{RGBD-cameras}
% TODO: RGBD, 

RGBD-cameras, short for \emph{Red-Green-Blue-Depth}-camera, is a type of low-cost camera commonly used for robot vision. The concept became widely popular with the release of the Microsoft Kinect in late 2010. \par

These cameras consist of two separate parts: one normal color-based camera, and one infra-red sensor with accompanying projector. The sensing consists of projecting a deterministic pattern onto the scene, and then unprojecting them by comparing to previously captured patterns at known depths. By interpolating through these patterns, a full depth-image is generated.  
\subsection{Noise}
% TODO: Noise and smoothing; object identification
A common problem in any type of sensing is the introduction of noise into the system. This noise can come from many sources, and be predictable or unpredictable. Examples of noise sources could be frequency hum from electric circuits, flickering lights, air pollution or pure inaccuracy. This noise can skew the results of sensors that make algorithm much more error prone. \par

There are many approaches to reduce noise. Proper calibration and good testing environments is a good start, but this can only reduce external noise. Internal noise of the sensor needs to be analyzed and minimized on a much lower-level such as by using specially constructed algorithms. For sensors, that generate some sort of sequence one very naive (but nonetheless effective) approach is the use of smoothing. \par

\section{Implementation}
The purpose of this exercise is to calibrate an RGBD-camera and investigate its characteristics. Then, several smoothing algorithms shall be evaluated for the depth data.
\subsection{Hardware and environment}
This exercise was performed using an \emph{ASUS Xtion Pro}. The camera was connected over \emph{USB2} to a laptop running Linux kernel 4.2.5. The communication to the camera is done using the \emph{Robot Operating System} [ROS] version \emph{Indigo Igloo}. All packages used are compiled directly from GitHub development branch for Indigo Igloo. \par
Other software used includes the OpenCV libraries, version 2.4.12.2-1.
% TODO: Tom
\subsection{Camera setup}
% TODO: Marek : Screenshots, short introduction
As a first step we had to install \emph{openmi2} package for the corresponding version of ROS. This package contains drivers for the Asus camera. After installing the package we connected the camera to the USB 2.0 port on the laptop. We made sure that our system can recognize the camera through command \emph{lsusb} and after that we run the openmi2 package using \emph{roslaunch openmi2_launch openmi2.launch}. During the first launch there was a warning about no calibration file found, but we didn't need it yet for the camera testing. After running openmi2 we run in the new terminal window command \emph{rosrun rviz rviz}, which we used for inspecting the topics where camera publishes its data. In the RVIZ window we need to set global option Fixed Frame to camera_link in order to see the camera data. We need to create several visualizations to see what camera publishes on each topic. There are 4 major types of topics with different subtopics that can be visualized using RVIZ. There'are topics for depth image, depth registered image, rgb image and ir image.  \par
\begin{table}[h!]
  \centering
  \caption{Camera ROS topics}
  \label{tab:table1}
  \begin{tabular}{ccc}
    \toprule
    Topic & Description & Data preview\\
    \midrule
    topic & info & preview\\
    topic & info & preview\\
    topic & info & preview\\
    topic & info & preview\\
    topic & info & preview\\
    topic & info & preview\\
    \bottomrule
  \end{tabular}
\end{table}
\subsection{ROS setup}
Instead of using RVIZ to view the data as before, a custom ROS-node can be used. As there are three types of data - color image, depth image, and pointcloud, there are three listeners setup to receive this data. An example of the data on these topics can be found in the embedded files below. \par

\begin{center}
  \attachfile[color=0 0 0,icon=Paperclip]{pcloud.pcd}{{  }Pointcloud file}{   }
  \attachfile[color=0 0 0,icon=Paperclip]{rgbbmp.yml}{{  }RGB-image file}
\end{center}
%\attachfile{depthimage.yml}{Depth-image file}
% TODO: Tom : Sample files, point cloud & images
\subsection{Camera calibration}
  \lstinputlisting[language=yaml]{camera.yaml} 
% TODO: Marek : Calibration file & process
	In this part we were supposed to calibrate the camera which requires to setup internal parameters. After that we should be able to get not distorted image from the camera and right distance measurement. \par
	As a first step we had to install \emph{camera_calibration} package. After that we listed all topics where camera publishes its data and we choosed topic /camera/rgb/image_raw. Before running calibration process, we need to measure how many squares are in the checkerboard pattern and what is the length of one square. Using command \emph{rosrun camera_calibration cameracalibrator.py --size=6x10 --square=0.065 image:=/camera/rgb/image_raw camera:=/camera/rgb --approximate=0.1} we run the calibration application with correctly setup parameters according to our measurements of the calibrating pattern. In the calibration window we could see image from the selected topic with detection of the pattern. We set the camera to the paralel position with the ground and pointed it to the wall. After that we kept moving and tilting the pattern all over the camera field of view until X, Y and Size progressbar showed long line and button Calibrate light up. This process took up about 2 minutes until we got good results and Calibration button lighted up. We pressed the button and then waited for few minutes until the application was able to compute the right parameters for our camera. After that buttons Save and Upload lighted up. We saved the calibration file and tried to upload it to the camera firmware, but for some reason that function didn't worked properly and we had to set the calibration file as a parameter of the openmi2 package. The calibration file was saved into ost.txt file in the tmp directory and we needed to convert it to yaml file. In order to do that, we used command \emph{rosrun camera_calibration_parsers convert  ost.txt camera.yaml} which converted the calibration file into yaml file which we could use for the openmi2 package. \par
	To check the calibration results we tried to point the camera at some straight lines (wall corners, table etc.) and checked if they got distorted around the edges. That showed that the straight lines stayed as a straight lines, so the calibration was successful. To verify if the calibration worked in the quantitative manner we run the code from the Task 4: Noise characterization, that measured average and standard deviation for the distance in the small window. Then we pointed the camera to the objects in different distances and checked if the mean of the distance is the same as the reality, which mostly was within the range of the camera. \par
	
  \subsection{Noise characterization}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{figures/20x20-plot.png}
    \captionof{figure}[Mean and variance: 20x20 window]{\label{fig:20x20} The error and variance for a 20x20 pixel window at the center.}
  \end{figure}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{figures/plot40x40.png}
    \captionof{figure}[Mean and variance: 40x40 window]{\label{fig:40x40} The error and variance for a 40x40 pixel window at the center.}
  \end{figure}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{figures/plotwindowsizes.png}
    
    \captionof{figure}[Mean and variance: 50cm, varied window sizes]{\label{fig:variedwindow} The error and variance at 50cm for various window sizes.}
  \end{figure}

% TODO: Tom : Show plots; Measuring setup;
% TODO: Both : Analysis of plots
\subsection{Noise filtering}
% TODO: Tom : Implementation, challenges (NAAAN), push code with visible images
% TODO: Marek : Target scene (point 3)
% TODO: Both : Analysis

\section{Results}
% Both : Summary...

\bibliography{References}
\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
