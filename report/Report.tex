\documentclass[11pt]{article}
%%%
% TODO TOM: Add theory and motivation intro
%% Finish smoothing comparison Comment code


%% TODO MAREK: Results/conclusion Language check

%%%
%% Separate file for preamble with macros and stuff
\include{preamble}

%% For make-title
\title{Laboration 2: RGBD-cameras\\ {\small Sensors and Sensing}} \author{Marek
  Beƒçica, Tom Olsson} \date{\today}

\begin{document}
\maketitle %Title area
\begin{center}
  \emph{All code for this exercise can be found at \\ \url{https://github.com/tgolsson/sensors-laboration2-xtion}} \\ \vspace{10pt}
  \textbf{\Large In some of images and plots the ``variance'' is wrongly labeled as
    ``standard deviation''. This is just a typesetting error in the images, and
    not the actual values. All values are variances.}
\end{center}
\tableofcontents
\lstlistoflistings % List of all code snippets
\listoffigures % List of all figures
\listoftables \lstset{
  matchrangestart=t} %initialise the linerange-macro for \lstinput...
\section{Theory and motivation}
Robot vision is an important subject in the development of autonomous
robots. However, as with all sensors, cameras suffer from noise which makes
interpretation of the images hard, which in turn reduces the success of
processing such as image identification.

\subsection{RGBD-cameras}
RGBD-cameras, short for \emph{Red-Green-Blue-Depth}-camera, is a type of
low-cost camera commonly used for robot vision. The concept became widely
popular with the release of the Microsoft Kinect in late 2010. \par

These cameras consist of two separate parts: one normal color-based camera, and
one infrared sensor with accompanying projector. The sensing consists of
projecting a deterministic pattern onto the scene using an infrared emitter, and
then unprojecting by comparing the image to previously captured patterns at
known depths. By interpolating through these patterns throughout the full scene, a depth-image is
generated.


\subsection{Noise}

A common problem in any type of sensing is the introduction of noise into the
system. This noise can come from many sources, and be predictable or
unpredictable. Examples of noise sources could be frequency hum from electric
circuits, flickering lights, air pollution or pure inaccuracy. This noise can
skew the results of sensors and make execution more error prone. \par

There are many approaches to reduce noise. Proper calibration is a good start,
but this can only reduce some types external noise. Internal noise of the sensor
needs to be analyzed and minimized on a much lower-level such as by using
specially constructed algorithms. For sensors that generate some sort of
sequence, one very naive (but nonetheless effective) approach is the use of
smoothing. \par

\section{Implementation}
The purpose of this exercise is to calibrate an RGBD-camera and investigate its
characteristics. Then, several smoothing algorithms shall be evaluated to reduce
noise in the depth sensor.

\subsection{Hardware and environment}
This exercise was performed using an \emph{ASUS Xtion Pro}. The camera was
connected over \emph{USB2} to a laptop running Linux kernel 4.2.5. The
communication to the camera is done using the \emph{Robot Operating System}
[ROS] version \emph{Indigo Igloo}. All ROS packages used are compiled directly
from GitHub development branch for Indigo Igloo. \par
Other software used includes the OpenCV libraries, version 2.4.12.2-1.

\subsection{Camera setup}
As a first step we installed \emph{openni2} package for ROS Indigo Igloo.
This package contains drivers for the ASUS camera. After
installing the package we connected the camera to the USB 2.0 port on the
laptop, and sure that the system can recognize the camera through command
\texttt{lsusb}. After that we ran the openni2 package using \texttt{roslaunch
  openni2\_launch openni2.launch}. \par

After starting \texttt{openni2} we ran the command \texttt{rosrun rviz rviz},
which we used to visually inspect the topics published by the camera through \emph{openni2}. In the
RVIZ window we needed to set the global option \textbf{Fixed Frame} to
\textbf{camera\_link} in order to see the camera data. \par

We created several visualizations to see what camera publishes on each
topic. There are four major types of topics with different subtopics that can be
visualized using RVIZ. There are topics for \textbf{depth image}, \textbf{depth
  registered image}, \textbf{RGB image} and \textbf{infrared image}. The
depth and depth registered data can also be used as pointclouds. The topics can be
seen in table \vref{tab:table1}. The subtopics are omitted, because they contain
the same data but filtered. \par
\noindent
{
  \begin{tabularx}{1\textwidth}{lXc}  
      \toprule
      Topic & Description & Data preview\\
      \midrule
      \texttt{/camera/depth/image\_raw} &
Depth image & 
\raisebox{-.5\height}{      \includegraphics[width=0.2\textwidth]{figures/depth-image-raw.png}}\\
      
      \texttt{/camera/depth/points} &

                                      Point cloud & 
\raisebox{-.5\height}{      \includegraphics[width=0.2\textwidth]{figures/depth-image-raw-pointcloud.png}}\\
      
      \texttt{/camera/depth\_registered/image\_raw} &
                                                      Combination of depth and
                                                      RGB images& 
\raisebox{-.5\height}{      \includegraphics[width=0.2\textwidth]{figures/depth_registered-image-raw.png}}\\
      
      \texttt{/camera/depth\_registered/points} &
                       Same as depth registered, but as a pointcloud   & 
\raisebox{-.5\height}{      \includegraphics[width=0.2\textwidth]{figures/depth_registered-image-raw-pointcloud.png}}\\
      
      \texttt{/camera/ir/image} &
      The infrared pattern captured by the sensor & 
\raisebox{-.5\height}{      \includegraphics[width=0.2\textwidth]{figures/ir-image-raw.png}}\\
      
      \texttt{/camera/rgb/image\_raw} &
      RGB image & 
\raisebox{-.5\height}{      \includegraphics[width=0.2\textwidth]{figures/rgb-image-raw.png}}\\
      \bottomrule
    \end{tabularx}
    \captionof{table}[ROS: Camera topics]{\label{tab:table1}Camera ROS topics}
  }



\subsection{ROS setup}
Instead of using RVIZ to view the data as before, a custom ROS-node can be
used. As there are three types of data, color image, depth image, and point
cloud, there are three listeners setup to receive this data. An example of the
data on these topics can be found in the embedded files below\footnote{If these
  embedded files do not work, such as if you use Adobe Acrobat, please download
  them from the \texttt{report} folder in the GitHub repository. Verified to
  work with Okular and Evince on Linux.}. The \texttt{depth image} and
\texttt{color image} are stored in the OpenCV \texttt{y[a]ml} format, and the
point cloud is stored in the \emph{Point Cloud Library} [PCL] \texttt{pcd}
format.\par

\begin{center}
  \attachfile[color=0 0 0,icon=Paperclip]{pcloud.pcd}{{ }Point cloud file}{ }
  \attachfile[color=0 0 0,icon=Paperclip]{rgbbmp.yml}{{ }RGB-image file}{ }
  \attachfile[color=0 0 0,icon=Paperclip]{rgbdepth.yml}{{ }Depth-image file}
\end{center}

\subsection{Camera calibration}

Next, the camera was calibrated by setting its internal parameters in openni. This is required to get an undistorted image
from the camera and therefore the right distance measurement. \par

As a first installed the ROS \emph{camera\_calibration} package. Before running
the calibration process, we measured the number of squares on the checkerboard
and their size. The checkerboard used had 11 squares on one axis, and 7 on the
other; and each side was 6.5 cm long.  Using the command \texttt{rosrun
  camera\_calibration cameracalibrator.py --size=6x10 --square=0.065}\\\texttt{
  image:=/camera/rgb/image\_raw}{ } \texttt{camera:=/camera/rgb
  --approximate=0.1}, we could then start the calibration process. As shown in
the command, we used the \texttt{rgb/image\_raw} topic to do our calibration.\par

In the calibration window we could see image from the selected topic with the
pattern highlighted. We set the camera be parallel with the floor and pointed it
at the wall. After that continuously moved and tilted the pattern inside the
camera field of view until X, Y and Size progress bar filled up and, and the
camera could be calibrated from the captured data. The data collection took
about 2-3 minutes to complete.  The software is supposedly able to store the
calibration directly to the camera; but this did not work. Instead, we saved the
calibration file and tried to upload it to the camera firmware, but for some
reason that function didn't work. Instead, we later passed the calibration file
to the openni software as a command line argument. \par
        
The calibration file was saved into \emph{out.txt} file in the \textbf{tmp}
directory before being converted into a YAML file. To do that, we used
the command \texttt{rosrun camera\_calibration\_parsers convert /tmp/out.txt camera.yaml},
which converted the calibration file into a YAML file in the format used by openni. The calibration file is shown in listing \vref{lst:calib}. \par
        
To check the calibration results we pointed the camera at nearby straight lines
(corners, tables etc.) and visually checked if they got distorted around the
edges (fisheye lens effect). That showed that the straight lines stayed as
straight lines, so the calibration was successful. To quantitatively verify the
calibration, we took a screenshot of the camera image, and fitted a line onto
one of the straight lines in the picture. As this was possible to do without errors, it shows
that the line is not only visually straight but also perfectly straight. Had
this not been the case, it would have been possible to measure the error
between the perfect line and the camera line but this was found to be
unnecessary. \par

We also noticed that there are some objects, such as lights and reflective 
surfaces, that ruin the measurements and return wrong distance 
data. \par
	
\lstinputlisting[language=yaml,caption={The calibration file for the camera},
label={lst:calib}]{camera.yaml}

\subsection{Noise characterization}

The sensors used in the camera suffer from noise, and the first requirement for
improvement is to objectively quantify the error. First, 5 measurements of 10
seconds each were recorded with \texttt{rosbag}. The measurements were taken
\emph{50}, \emph{100}, \emph{150}, \emph{200}, and \emph{300} cm from a white
wall. The camera was placed on a table, so that the camera normal was
perpendicular to the wall. This ensures that all later noise and smoothing
comparisons are made on the same data. 

The recorded data can be downloaded here: \url{
  https://drive.google.com/folderview?id=0B4HtjhS_-Y3ZZEN4NGctMk9DSDg&usp=sharing}.\par

First, the errors were measured at varying distance with set window
sizes. Fig. \vref{fig:20x20} shows the error and variances for a 20x20 window,
and fig. \vref{fig:40x40} shows the same for a 40x40 window. The most obvious
pattern can be seen in the right-most plot in the two figures: there is a very
strong correlation between distance, and both variance and error. This has been
shown before, and is logical: the further the distance, the more
things can affect the measurements. \par

Another interesting pattern is that the mean error barely changes between the
two windows; however, a larger window reduces the variance significantly. This
is not so much related to reduced errors, but a result of the definition of a
variance. If the errors are assumed to adhere to some sort of distribution
$X \in \left(-\infty, \infty \right)$, then $\frac{1}{\left|X\right|}\sum_{X_1}^{X_\infty} = \bar{X}$,
i.e., the static error. From this it follows that the variance will be
on same order of magnitude as the noise in the signal. However, if too few
points are sampled the sum will only approximate the mean, and the variance will
become unstable. There is therefore a trade-off between window size and
computational cost; i.e. while a larger window may be preferable for preciseness
this may be computationally expensive, especially for more complex filters. \par

\begin{figure}[ht]
  \includegraphics[width=1\textwidth]{figures/20x20-plot.png}
  \captionof{figure}[Mean and variance: 20x20 window]{\label{fig:20x20} The
    error and variance for a 20x20 pixel window at the center.}
\end{figure}

\begin{figure}[ht]
  \includegraphics[width=1\textwidth]{figures/plot40x40.png}
  \captionof{figure}[Mean and variance: 40x40 window]{\label{fig:40x40} The
    error and variance for a 40x40 pixel window at the center.}
\end{figure}

Moving on, the distance can instead be kept constant while the window size is
varied. This is shown for 50 cm in fig.  \vref{fig:variedwindow}. Here, the
effect mentioned above with variance is amplified a lot; and error and variance
become very stable for the large window sizes; and in the case of variance also
very small. There is a visible artifact between samples 0-60 which has much
smaller values than the other values. As most of the values are higher, these
smaller values can be discarded. \par


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{figures/plotwindowsizes.png}
  \captionof{figure}[Mean and variance: 50 cm, varied window
  sizes]{\label{fig:variedwindow} The error and variance at 50 cm for various
    window sizes.}
\end{figure}

The cleaned data shown in fig. \vref{fig:variedwindow2} highlights how stable
the mean is for the larger window sizes. It also shows that something is
interfering in the 40x40 measurement, though it is hard to pinpoint what. One
(hypothetical) possibility is that there is a small patch of very noisy values
that get introduced when the window size increases, and as the window size is
still quite small it has a large impact on the variance. \par

One last phenomenon can be seen, and this is that the mean increases with window
sizes. This is caused by the increase in angle towards the edges of the
window. As all distances are measured from the sensor, this causes them to
increase slightly. The increase is approximately $\frac{1}{\cos \alpha}$, where
$\alpha$ is the angle relative to the camera normal.\par

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{figures/plot2bywindowsize.png}
  \captionof{figure}[Mean and variance: 50 cm, varied window sizes,
  cleaned]{\label{fig:variedwindow2} The error and variance at 50 cm for various
    window sizes with faulty data removed.}
\end{figure}

\subsection{Noise filtering}

To improve the input depth image five different filters were implemented for
smoothing. The first three filters; \emph{median}, \emph{gaussian}, and
\emph{bilateral}, come from the OpenCV library. The last two filters, in the
temporal domain, were implemented manually in C++ and are shown in listing
\vref{lst:temporal}. A large issue when using the filters was handling the large
number of invalid values in the input. In some situations, a majority of the
elements were NaN, which obviously makes it hard to draw any conclusions from
the data. This caused particular problems with the bilateral filter, which
suffers a segmentation fault when there are too many NaN values in the
input. \par

\lstinputlisting[float,caption={The temporal smoothing
  algorithm},label={lst:temporal},language=C++,linerange={269-320}]{../asus_node/src/asus_node.cc}

In order to solve this issue, all NaN-values in the input are set to $10000$
before being passed to the bilateral filter. By the edge preserving property of
bilateral filtering, this should reduce the impact they have on the actual
filtering process. After the filtering is done, every cell which was set to 10
000 is set to 0; and the mean and variance is calculated on all non-zero
elements. \par

Another solution we considered - but did not implement - was to use local
median-filtering at each \texttt{NaN}. We deemed this to be a very
computationally expensive process, but could possibly yield better results. \par

The result of filtering our depth-images is seen below in
fig. \ref{fig:plots20x20filtered} and \vref{fig:plots40x40filtered}. Just as in
the previous section, the unreliable data at the start is removed to clean up
the results. As can be seen, the two temporal smoothing algorithms far
outperform the other algorithms at 50 cm for both window sizes, though the
difference is very small. However, when increasing the distance up to 300 cm
this changes as shown in fig. \vref{fig:300filt}. In this case, the gaussian 
and bilateral filter have a better performance on both variance and mean error. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{figures/plot20x20filtered.png}
  \captionof{figure}[Filtered data: 20x20, 50 cm]{\label{fig:plots20x20filtered}
    Mean and variance for the filters at 50 cm with 20x20 window size.}
\end{figure}
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1\textwidth]{figures/plot40x40filtered.png}
    \captionof{figure}[Filtered data: 40x40, 50
    cm]{\label{fig:plots40x40filtered} Mean and variance for the filters at 50
      cm with 40x40 window size.}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1\textwidth]{figures/filtered300cm.png}
    \captionof{figure}[Filtered data: 20x20, 300 cm]{\label{fig:300filt} Mean
      and variance for the filters at 300 cm with 20x20 window size.}
  \end{center}
\end{figure}

To visualize the smoothing algorithms we set up a scene with a small box in the
middle, and a bigger box behind it. This allowed us to clearly see the edges of
the smaller box through the camera. As we can see in the original image
(fig. \vref{fig:refimage}) there is a lot of noise and the edges are not as
clear as they should be. The used filters should smooth out the noisy edges to be able
to get better measurements. The result of filtering the area around the smaller boxis shown in fig. \vref{fig:filterswindow}. \par

The filtered area is 80x80 px in the middle of the camera data. The first image
is the original without any filters applied, but with NaN values set to 0. The
second image is filtered with a gaussian kernel. This filter blurs the image and
reduce details, which results in smoother edges and reduced noise. The last
picture on the upper row is median filtered. We can see that median filter
preserves the edges while removing some noise. \par

The first picture in the second row has been filtered using a bilateral
filter. This filter also smooths, while preserving large intensity
differences. The second picture in the same row is the average over 10
sequential images. This method reduces noise which appears only in few pictures
or reduce effect of moving objects, that could affect the measurements, as well
as oscillations in measurements. Last picture is the median of 10 images applied
per pixel. Effect of that method is similar to the previous one, but it
preserves more details. The effect of temporal smoothing was so strong that a
hand could be repeatedly moved quickly in and out of the camera field, without affecting
the measurements.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{figures/reference_rgb_depth.png}
  \captionof{figure}[Reference scene: RGB and depth image]{\label{fig:refimage}
    Reference image for demonstrating filtering effects.}
\end{figure}
\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{figures/applied_filters_center.png}
  
  \captionof{figure}[Reference scene: Filtered 80x80 center
  window]{\label{fig:filterswindow} Different filters applied
    to the center 80x80 px of the reference image. From the top left corner - original
    image, gaussian, median, bilateral, temporal average, temporal median.}
\end{figure}

\section{Results}
During this exercise we could see that data received from this ASUS camera is 
very noisy and because of this they might not be completely reliable. On top of 
that we found that when viewing surfaces that emit or absorb light, we 
get wrong distance values. \par

Before we the calibration we could see that camera suffers from the fisheye lens
effect, that distorted objects near the edges of the field of view. The
calibration process reduced this effect to minimum and resulted in straight
undistorted lines. \par

In order to reduce noise we tried several methods. The first method was to
increase the window from which we computed the mean distance and variance. Our
results showed that bigger window give better results, but in the real time
usage this can be computationally expensive. We also verified previous results
that long distances can be detrimental to the camera performance. \par

The second method we tried was to use filters to smooth the image and as a
result reduce the noise. As our results showed, the best performing filter were
temporal filters on the 50 cm distance, but gaussian and bilateral filter at 300
cm distance. This could be related to the increasing variance and mean at high
distances, which causes self-similarity to become stronger than
sequence-similarity of the images. \par

The results are not conclusive for other situations than a white wall, because
the camera, and by extent the filters, will behave very differently in different
environments. We have noted that different surfaces will drastically change the
performance, and we noticed that flickering lights interfered with our
measurements. Because of this, we can't say that using these filters are the
best for every situation, despite the fact that it worked best in our
setup. \par


\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
